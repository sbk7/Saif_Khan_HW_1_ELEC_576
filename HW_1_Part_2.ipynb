{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM4NcfQHgMNMvQqmoxoc2Yh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nPnJh4y691q-","executionInfo":{"status":"ok","timestamp":1664953477769,"user_tz":300,"elapsed":1282,"user":{"displayName":"Saif Khan","userId":"06529519966863140814"}},"outputId":"457f62f9-c05e-41ea-d8fb-9143c49a8a9b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!ls \"/content/drive/My Drive/Colab Notebooks\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gb31hkbn-nzb","executionInfo":{"status":"ok","timestamp":1664953479151,"user_tz":300,"elapsed":227,"user":{"displayName":"Saif Khan","userId":"06529519966863140814"}},"outputId":"438efbfc-ee3b-406b-87b7-1cd43576f85c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["dcn_mnist-1.py\t   HW_1_Part_2.ipynb  __pycache__\n","HW_1_Part_1.ipynb  input_data.py      three_layer_neural_network.py\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.insert(0,'/content/drive/My Drive/Colab Notebooks')\n","print(sys.path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ZbPRMof_J5G","executionInfo":{"status":"ok","timestamp":1664953479920,"user_tz":300,"elapsed":6,"user":{"displayName":"Saif Khan","userId":"06529519966863140814"}},"outputId":"c725b163-421e-44b9-cb7b-e9fe8a1b5195"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/drive/My Drive/Colab Notebooks', '/content/drive/My Drive/Colab Notebooks', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","src = list(files.upload().values())[0]\n","open('input_data.py','wb').write(src)\n","import input_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"h3AYb3RA_sYL","executionInfo":{"status":"ok","timestamp":1664953486321,"user_tz":300,"elapsed":4938,"user":{"displayName":"Saif Khan","userId":"06529519966863140814"}},"outputId":"9df83ef3-8703-458a-ae6e-d639aa8d782f"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-6d3d4807-0ce7-4199-b1e3-3045d99aafa0\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-6d3d4807-0ce7-4199-b1e3-3045d99aafa0\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving input_data.py to input_data (1).py\n"]}]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"93M8CyY69mD0","executionInfo":{"status":"ok","timestamp":1664953491417,"user_tz":300,"elapsed":257,"user":{"displayName":"Saif Khan","userId":"06529519966863140814"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"V4g1zI5IoJOf","executionInfo":{"status":"error","timestamp":1664953961024,"user_tz":300,"elapsed":7136,"user":{"displayName":"Saif Khan","userId":"06529519966863140814"}},"outputId":"f51921ff-ef8d-4b8f-d249-48d3ddf9c9c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting MNIST_data/train-images-idx3-ubyte.gz\n","Extracting MNIST_data/train-labels-idx1-ubyte.gz\n","Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n","Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n","step 0, training accuracy 0.16\n"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1360\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1361\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1454\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'y__3' with dtype float and shape [?,10]\n\t [[{{node y__3}}]]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-e9912ab81971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-e9912ab81971>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# Update the events file which is used to monitor the training (in this case,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# only the training loss is monitored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0msummary_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1371\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1394\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1396\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'y__3' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n      self.run()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n      user_expressions, allow_stdin,\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-8-2256c2625fde>\", line 254, in <module>\n      main()\n    File \"<ipython-input-8-2256c2625fde>\", line 109, in main\n      y_ = tf.placeholder(tf.float32, [None, 10], name='y_')\nNode: 'y__3'\nYou must feed a value for placeholder tensor 'y__3' with dtype float and shape [?,10]\n\t [[{{node y__3}}]]\n\nOriginal stack trace for 'y__3':\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-2256c2625fde>\", line 254, in <module>\n    main()\n  File \"<ipython-input-8-2256c2625fde>\", line 109, in main\n    y_ = tf.placeholder(tf.float32, [None, 10], name='y_')\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 3301, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 6894, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3784, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 2175, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n"]}],"source":["import os\n","import time\n","\n","import tensorflow as tf\n","\n","if (tf.__version__.split('.')[0] == '2'):\n","    import tensorflow.compat.v1 as tf\n","\n","    tf.disable_v2_behavior()\n","\n","# Load MNIST dataset\n","#import input_data\n","\n","\n","import input_data\n","mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n","\n","sess = tf.InteractiveSession()\n","\n","\n","def weight_variable(shape):\n","    '''\n","    Initialize weights\n","    :param shape: shape of weights, e.g. [w, h ,Cin, Cout] where\n","    w: width of the filters\n","    h: height of the filters\n","    Cin: the number of the channels of the filters\n","    Cout: the number of filters\n","    :return: a tensor variable for weights with initial values\n","    '''\n","\n","    # IMPLEMENT YOUR WEIGHT_VARIABLE HERE\n","    initial = tf.truncated_normal(shape, stddev=0.1)\n","    W = tf.Variable(initial)\n","    return W\n","\n","\n","def bias_variable(shape):\n","    '''\n","    Initialize biases\n","    :param shape: shape of biases, e.g. [Cout] where\n","    Cout: the number of filters\n","    :return: a tensor variable for biases with initial values\n","    '''\n","\n","    # IMPLEMENT YOUR BIAS_VARIABLE HERE\n","    initial = tf.constant(0.1, shape=shape)\n","    b = tf.Variable(initial)\n","    return b\n","\n","\n","def conv2d(x, W):\n","    '''\n","    Perform 2-D convolution\n","    :param x: input tensor of size [N, W, H, Cin] where\n","    N: the number of images\n","    W: width of images\n","    H: height of images\n","    Cin: the number of channels of images\n","    :param W: weight tensor [w, h, Cin, Cout]\n","    w: width of the filters\n","    h: height of the filters\n","    Cin: the number of the channels of the filters = the number of channels of images\n","    Cout: the number of filters\n","    :return: a tensor of features extracted by the filters, a.k.a. the results after convolution\n","    '''\n","\n","    # IMPLEMENT YOUR CONV2D HERE\n","    h_conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n","\n","    return h_conv\n","\n","\n","def max_pool_2x2(x):\n","    '''\n","    Perform non-overlapping 2-D maxpooling on 2x2 regions in the input data\n","    :param x: input data\n","    :return: the results of maxpooling (max-marginalized + downsampling)\n","    '''\n","\n","    # IMPLEMENT YOUR MAX_POOL_2X2 HERE\n","    h_max = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    return h_max\n","\n","\n","def stats_summary(var_name, var):\n","    ##monitor the statistics (min, max, mean, standard deviation, histogram)\n","    tf.summary.scalar(\"Min__\" + var_name, tf.reduce_min(var))\n","    tf.summary.scalar(\"Max__\" + var_name, tf.reduce_max(var))\n","    tf.summary.scalar(\"Mean__\" + var_name, tf.reduce_mean(var))\n","    tf.summary.scalar(\"Std_Dev__\" + var_name, tf.math.reduce_std(var))\n","    tf.summary.histogram(\"Histo__\" + var_name, var)\n","\n","\n","def main():\n","    # Specify training parameters\n","    result_dir = './results/'  # directory where the results from the training are saved\n","    max_step = 5500  # the maximum iterations. After max_step iterations, the training will stop no matter what\n","    test_dir = result_dir + 'test/'\n","    val_dir = result_dir + 'val/'\n","\n","    start_time = time.time()  # start timing\n","\n","    # FILL IN THE CODE BELOW TO BUILD YOUR NETWORK\n","\n","    # placeholders for input data and input labeles\n","    x = tf.placeholder(tf.float32, [None, 784], name='x')\n","    y_ = tf.placeholder(tf.float32, [None, 10], name='y_')\n","\n","    # Store Accuracies\n","    val_accuracy_ = tf.placeholder(tf.float32, shape=())\n","    test_accuracy_ = tf.placeholder(tf.float32, shape=())\n","\n","    # reshape the input image\n","    x_image = tf.reshape(x, [-1, 28, 28, 1])\n","\n","    # first convolutional layer\n","    W_conv1 = weight_variable([5, 5, 1, 32])\n","    b_conv1 = bias_variable([32])\n","    input_1 = conv2d(x_image, W_conv1) + b_conv1\n","    h_conv1 = tf.nn.relu(input_1)\n","    h_pool1 = max_pool_2x2(h_conv1)\n","\n","    # second convolutional layer\n","    W_conv2 = weight_variable([5, 5, 32, 64])\n","    b_conv2 = bias_variable([64])\n","    input_2 = conv2d(h_pool1, W_conv2) + b_conv2\n","    h_conv2 = tf.nn.relu(input_2)\n","    h_pool2 = max_pool_2x2(h_conv2)\n","\n","    # densely connected layer\n","    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n","    b_fc1 = bias_variable([1024])\n","    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n","    input_fc1 = tf.matmul(h_pool2_flat, W_fc1) + b_fc1\n","    h_fc1 = tf.nn.relu(input_fc1)\n","\n","    # dropout\n","    keep_prob = tf.placeholder(tf.float32)\n","    h_fc1_drop = h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n","\n","    # softmax\n","    W_fc2 = weight_variable([1024, 10])\n","    b_fc2 = bias_variable([10])\n","    y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2, name='y')\n","\n","    # FILL IN THE FOLLOWING CODE TO SET UP THE TRAINING\n","\n","    # setup training\n","    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n","    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n","    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n","\n","    # Add a scalar summary for the snapshot loss.\n","    tf.summary.scalar(cross_entropy.op.name, cross_entropy)\n","    # Build the summary operation based on the TF collection of Summaries.\n","    # Layer-1\n","    stats_summary(\"W_conv1\", W_conv1)\n","    stats_summary(\"b_conv1\", b_conv1)\n","    stats_summary(\"input_1\", input_1)\n","    stats_summary(\"h_conv1\", h_conv1)\n","    stats_summary(\" h_pool1\", h_pool1)\n","    # Layer-2\n","    stats_summary(\"W_conv2\", W_conv2)\n","    stats_summary(\"b_conv2\", b_conv2)\n","    stats_summary(\"input_2\", input_2)\n","    stats_summary(\"h_conv2\", h_conv2)\n","    stats_summary(\"h_pool2\", h_pool2)\n","    # densely connected layer\n","    stats_summary(\"W_fc1\", W_fc1)\n","    stats_summary(\"b_fc1\", b_fc1)\n","    stats_summary(\"h_pool2_flat\", h_pool2_flat)\n","    stats_summary(\"input_fc1\", input_fc1)\n","    stats_summary(\"h_fc1\", h_fc1)\n","    # Output layer - Softmax\n","    stats_summary(\"W_fc2\", W_fc2)\n","    stats_summary(\"b_fc2\", b_fc2)\n","    stats_summary(\"y_conv \", y_conv)\n","\n","    summary_op = tf.summary.merge_all()\n","    summary_op_test = tf.summary.scalar('test_accuracy', test_accuracy_)\n","    summary_op_val = tf.summary.scalar('val_accuracy', val_accuracy_)\n","\n","    # Add the variable initializer Op.\n","    init = tf.initialize_all_variables()\n","\n","    # Create a saver for writing training checkpoints.\n","    saver = tf.train.Saver()\n","\n","    # Instantiate a SummaryWriter to output summaries and the Graph.\n","    summary_writer = tf.summary.FileWriter(result_dir, sess.graph)\n","    # Val test\n","    summary_writer_test = tf.summary.FileWriter(test_dir, sess.graph)\n","    summary_writer_val = tf.summary.FileWriter(val_dir, sess.graph)\n","\n","    # Run the Op to initialize the variables.\n","    sess.run(init)\n","\n","    # run the training\n","    for i in range(max_step):\n","        batch = mnist.train.next_batch(50)  # make the data batch, which is used in the training iteration.\n","        # the batch size is 50\n","        if i % 100 == 0:\n","            # output the training accuracy every 100 iterations\n","            train_accuracy = accuracy.eval(feed_dict={\n","                x: batch[0], y_: batch[1], keep_prob: 1.0})\n","            print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n","\n","            # Update the events file which is used to monitor the training (in this case,\n","            # only the training loss is monitored)\n","            summary_str = sess.run(summary_op, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n","            summary_writer.add_summary(summary_str, i)\n","            summary_writer.flush()\n","\n","        # save the checkpoints every 1100 iterations\n","        if i % 1100 == 0 or i == max_step:\n","            checkpoint_file = os.path.join(result_dir, 'checkpoint')\n","            saver.save(sess, checkpoint_file, global_step=i)\n","\n","            feed_dict_test = {x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}\n","            test_accuracy = accuracy.eval(feed_dict=feed_dict_test)\n","            print('step %d, test accuracy %g' % (i, test_accuracy))\n","            summary_str_test = sess.run(summary_op_test, feed_dict={test_accuracy_: test_accuracy})\n","            summary_writer_test.add_summary(summary_str_test, i)\n","            summary_writer_test.flush()\n","\n","            feed_dict_val = {x: mnist.validation.images, y_: mnist.validation.labels, keep_prob: 1.0}\n","            val_accuracy = accuracy.eval(feed_dict=feed_dict_val)\n","            print('step %d, validation accuracy %g' % (i, val_accuracy))\n","            summary_str_val = sess.run(summary_op_val, feed_dict={val_accuracy_: val_accuracy})\n","            summary_writer_val.add_summary(summary_str_val, i)\n","            summary_writer_val.flush()\n","\n","        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})  # run one train_step\n","\n","    # print validation error\n","    print('validation accuracy %g' % accuracy.eval(feed_dict={\n","        x: mnist.validation.images, y_: mnist.validation.labels, keep_prob: 1.0}))\n","\n","    # print test error\n","    print(\"test accuracy %g\" % accuracy.eval(feed_dict={\n","        x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n","\n","    stop_time = time.time()\n","    print('The training takes %f second to finish' % (stop_time - start_time))\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","source":["tensorboard --logdir=results --bind_all"],"metadata":{"id":"I9g45EDF7Qej"},"execution_count":null,"outputs":[]}]}